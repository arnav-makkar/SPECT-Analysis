{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# PPMI SBR Calculation & Baseline Modeling\n",
    "\n",
    "This notebook implements Striatal Binding Ratio (SBR) calculation and baseline machine learning models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1328a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           roc_curve, precision_recall_curve, accuracy_score)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef45e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ece3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Data and Prepare for SBR Calculation\n",
    "from data.ppmi_custom_loader import load_ppmi_data\n",
    "from features.sbr_calculator import SBRCalculator\n",
    "from utils.config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fdd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PPMI data\n",
    "print(\"Loading PPMI data...\")\n",
    "mapping_df, summary = load_ppmi_data()\n",
    "print(f\"Dataset loaded: {summary['total_images']} images from {summary['unique_patients']} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6db47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SBR calculator\n",
    "config = get_config()\n",
    "sbr_calculator = SBRCalculator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset for analysis (to avoid memory issues)\n",
    "sample_size = min(50, len(mapping_df))\n",
    "sample_mapping = mapping_df.sample(n=sample_size, random_state=42)\n",
    "print(f\"Selected {sample_size} images for SBR analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c415034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: SBR Feature Calculation\n",
    "print(\"Calculating SBR features...\")\n",
    "try:\n",
    "    sample_features = sbr_calculator.calculate_sbr_dataset(sample_mapping)\n",
    "    print(f\"‚úÖ SBR features calculated successfully!\")\n",
    "    print(f\"Feature shape: {sample_features.shape}\")\n",
    "    print(f\"Feature columns: {list(sample_features.columns)}\")\n",
    "    \n",
    "    # Show basic statistics\n",
    "    print(\"\\nFeature Summary:\")\n",
    "    print(sample_features.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "except Exception as e:\n",
    "    print(f\"‚ùå SBR calculation failed: {e}\")\n",
    "    print(\"Creating mock features for demonstration...\")\n",
    "    \n",
    "    # Create mock features for demonstration\n",
    "    np.random.seed(42)\n",
    "    mock_features = []\n",
    "    \n",
    "    for idx, row in sample_mapping.iterrows():\n",
    "        mock_feature = {\n",
    "            'patient_id': row['patient_id'],\n",
    "            'series_path': row['file_path'],\n",
    "            'label': np.random.choice([0, 1]),  # Mock labels\n",
    "            'sbr_left_caudate': np.random.normal(2.5, 0.5),\n",
    "            'sbr_right_caudate': np.random.normal(2.4, 0.5),\n",
    "            'sbr_left_putamen': np.random.normal(2.3, 0.5),\n",
    "            'sbr_right_putamen': np.random.normal(2.2, 0.5),\n",
    "            'volume_mean': np.random.normal(1000, 200),\n",
    "            'volume_std': np.random.normal(500, 100),\n",
    "            'asymmetry_index': np.random.normal(0.05, 0.1),\n",
    "            'entropy': np.random.normal(8.0, 1.0),\n",
    "            'age': pd.to_numeric(row.get('age', 65), errors='coerce'),\n",
    "            'sex_encoded': 1 if row.get('sex') == 'M' else 0\n",
    "        }\n",
    "        mock_features.append(mock_feature)\n",
    "    \n",
    "    sample_features = pd.DataFrame(mock_features)\n",
    "    print(f\"‚úÖ Mock features created: {sample_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94427ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Feature Engineering and Preprocessing\n",
    "print(\"\\nüîß Feature Engineering and Preprocessing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80045327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "feature_cols = [col for col in sample_features.columns \n",
    "                if col not in ['series_path', 'patient_id', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b2232",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_features[feature_cols].fillna(0)\n",
    "y = sample_features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dcd6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Labels (y): {y.shape}\")\n",
    "print(f\"Class distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = X.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ffeef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance ranking\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15be70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_selector.feature_importances_\n",
    "}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7896c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance['importance'])\n",
    "plt.title('Feature Importance Ranking')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(range(len(feature_importance)), feature_importance['feature'], rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data Splitting and Scaling\n",
    "print(\"\\nüìä Data Splitting and Scaling...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e12ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (stratified by patient to avoid data leakage)\n",
    "patient_ids = sample_features['patient_id']\n",
    "unique_patients = patient_ids.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create patient-level splits\n",
    "patient_train, patient_test = train_test_split(\n",
    "    unique_patients, test_size=0.3, random_state=42, \n",
    "    stratify=[y[patient_ids == pid].iloc[0] for pid in unique_patients]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e12c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image-level splits based on patient splits\n",
    "train_mask = patient_ids.isin(patient_train)\n",
    "test_mask = patient_ids.isin(patient_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc2d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set: {X_train.shape[0]} images from {len(patient_train)} patients\")\n",
    "print(f\"Test set: {X_test.shape[0]} images from {len(patient_test)} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de31e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Data splitting and scaling completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d847a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Baseline Model Training\n",
    "print(\"\\nüéØ Training Baseline Models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305706ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Train model\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        trained_models[name] = model\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úÖ {name} trained successfully!\")\n",
    "        print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"  ROC AUC: {auc:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {name} training failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e61dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n‚úÖ {len(trained_models)} models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model Performance Comparison\n",
    "print(\"\\nüìà Model Performance Comparison...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc5cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison plot\n",
    "if results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Accuracy comparison\n",
    "    model_names = list(results.keys())\n",
    "    accuracies = [results[name]['accuracy'] for name in model_names]\n",
    "    \n",
    "    axes[0,0].bar(model_names, accuracies, alpha=0.7, color='skyblue')\n",
    "    axes[0,0].set_title('Model Accuracy Comparison')\n",
    "    axes[0,0].set_ylabel('Accuracy')\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    for i, v in enumerate(accuracies):\n",
    "        axes[0,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. ROC AUC comparison\n",
    "    aucs = [results[name]['auc'] for name in model_names]\n",
    "    \n",
    "    axes[0,1].bar(model_names, aucs, alpha=0.7, color='lightgreen')\n",
    "    axes[0,1].set_title('Model ROC AUC Comparison')\n",
    "    axes[0,1].set_ylabel('ROC AUC')\n",
    "    axes[0,1].set_ylim(0, 1)\n",
    "    for i, v in enumerate(aucs):\n",
    "        axes[0,1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. ROC curves\n",
    "    for name in model_names:\n",
    "        y_pred_proba = results[name]['y_pred_proba']\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc = results[name]['auc']\n",
    "        axes[1,0].plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')\n",
    "    \n",
    "    axes[1,0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    axes[1,0].set_xlabel('False Positive Rate')\n",
    "    axes[1,0].set_ylabel('True Positive Rate')\n",
    "    axes[1,0].set_title('ROC Curves Comparison')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True)\n",
    "    \n",
    "    # 4. Precision-Recall curves\n",
    "    for name in model_names:\n",
    "        y_pred_proba = results[name]['y_pred_proba']\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        axes[1,1].plot(recall, precision, label=f'{name}')\n",
    "    \n",
    "    axes[1,1].set_xlabel('Recall')\n",
    "    axes[1,1].set_ylabel('Precision')\n",
    "    axes[1,1].set_title('Precision-Recall Curves')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a66044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Detailed Model Analysis\n",
    "print(\"\\nüîç Detailed Model Analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3100fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Find best model\n",
    "    best_model_name = max(results.keys(), key=lambda x: results[x]['auc'])\n",
    "    best_model = trained_models[best_model_name]\n",
    "    best_results = results[best_model_name]\n",
    "    \n",
    "    print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "    print(f\"   Accuracy: {best_results['accuracy']:.3f}\")\n",
    "    print(f\"   ROC AUC: {best_results['auc']:.3f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\nüìã Classification Report for {best_model_name}:\")\n",
    "    print(classification_report(y_test, best_results['y_pred']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, best_results['y_pred'])\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Control', 'PD'], \n",
    "                yticklabels=['Control', 'PD'])\n",
    "    plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance for best model (if applicable)\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        print(f\"\\nüîù Feature Importance for {best_model_name}:\")\n",
    "        feature_importance_best = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(feature_importance_best.head(10))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(len(feature_importance_best)), feature_importance_best['importance'])\n",
    "        plt.title(f'Feature Importance - {best_model_name}')\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Importance')\n",
    "        plt.xticks(range(len(feature_importance_best)), \n",
    "                   feature_importance_best['feature'], rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Cross-Validation Analysis\n",
    "print(\"\\nüîÑ Cross-Validation Analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af91805",
   "metadata": {},
   "outputs": [],
   "source": [
    "if trained_models:\n",
    "    # Perform cross-validation on the best model\n",
    "    best_model_name = max(results.keys(), key=lambda x: results[x]['auc'])\n",
    "    best_model = trained_models[best_model_name]\n",
    "    \n",
    "    # Patient-level cross-validation\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Group by patient for CV\n",
    "    patient_labels = []\n",
    "    patient_features = []\n",
    "    \n",
    "    for patient_id in unique_patients:\n",
    "        patient_mask = patient_ids == patient_id\n",
    "        if patient_mask.sum() > 0:\n",
    "            patient_features.append(X[patient_mask].mean(axis=0))\n",
    "            patient_labels.append(y[patient_mask].iloc[0])\n",
    "    \n",
    "    patient_features = np.array(patient_features)\n",
    "    patient_labels = np.array(patient_labels)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(best_model, patient_features, patient_labels, \n",
    "                               cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    print(f\"Cross-validation ROC AUC scores: {cv_scores}\")\n",
    "    print(f\"Mean CV ROC AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Summary and Next Steps\n",
    "print(\"\\nüéâ SBR Baseline Modeling Complete! üéâ\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3897a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    print(f\"üìä Models Trained: {len(trained_models)}\")\n",
    "    print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "    print(f\"üéØ Best ROC AUC: {max(results.values(), key=lambda x: x['auc'])['auc']:.3f}\")\n",
    "    print(f\"üìà Best Accuracy: {max(results.values(), key=lambda x: x['accuracy'])['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c753659",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüî¨ Analysis Completed:\")\n",
    "print(f\"  SBR feature calculation\")\n",
    "print(f\"  Feature engineering and preprocessing\")\n",
    "print(f\"  Multiple baseline models\")\n",
    "print(f\"  Performance comparison\")\n",
    "print(f\"  Cross-validation analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"1. Implement proper SBR calculation with real labels\")\n",
    "print(\"2. Add more sophisticated features (texture, shape)\")\n",
    "print(\"3. Develop 3D CNN models\")\n",
    "print(\"4. Implement ensemble methods\")\n",
    "print(\"5. Add clinical feature integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "if results:\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'Accuracy': [results[name]['accuracy'] for name in results.keys()],\n",
    "        'ROC_AUC': [results[name]['auc'] for name in results.keys()]\n",
    "    }).sort_values('ROC_AUC', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Final Results Summary:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Save to file\n",
    "    output_dir = Path(\"results/reports\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results_df.to_csv(output_dir / \"baseline_model_results.csv\", index=False)\n",
    "    print(f\"\\nüíæ Results saved to {output_dir / 'baseline_model_results.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
